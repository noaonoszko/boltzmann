{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/frontend/cont/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from dataset import AsyncSubsetFineWebEdu2Loader  # Custom dataset module\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "import asyncio\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_indices(model, seed, compression:int, device:str):\n",
    "    if compression == 1:\n",
    "        return {name: torch.arange(param.numel()).long().cpu() for name, param in model.named_parameters()}\n",
    "    \n",
    "    seed = int(hashlib.md5(str(seed).encode('utf-8')).hexdigest(), 16) % (2**32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    result = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        # Randomly select indices based on the compression factor\n",
    "        num_indices = max(1, int(param.numel() // compression))\n",
    "        indices = rng.choice(param.numel(), size=num_indices, replace=False)\n",
    "        result[name] = torch.from_numpy(indices).long().to(device)\n",
    "    return result\n",
    "\n",
    "async def run_sim(base_model, miner_models, optimizers, tokenizer, compression):\n",
    "    \"\"\"\n",
    "    Run a simulation step to compute cosine similarities between miner updates and base model gradients.\n",
    "\n",
    "    Args:\n",
    "        base_model (nn.Module): The base model.\n",
    "        miner_models (list of nn.Module): List of miner models.\n",
    "        optimizers (list of Optimizer): List of optimizers for the miner models.\n",
    "        tokenizer (Tokenizer): Tokenizer used for the models.\n",
    "\n",
    "    Returns:\n",
    "        similarity (list of float): List of total cosine similarities for each miner model.\n",
    "    \"\"\"\n",
    "    offset = int(time.time())\n",
    "    similarity = []\n",
    "    divergence = []\n",
    "    indices = get_indices(base_model, random.randint(0, 1000), compression=compression, device = base_model.device)\n",
    "    \n",
    "    # Compute average.\n",
    "    averages = {}\n",
    "    for name, param in base_model.named_parameters():\n",
    "        idx = indices[name]        \n",
    "        avg_param = torch.zeros_like(param).view(-1)[idx]\n",
    "        for miner_model in miner_models:\n",
    "            miner_param = dict(miner_model.named_parameters())[name]\n",
    "            avg_param += miner_param.view(-1)[idx]\n",
    "        avg_param /= len(miner_models)\n",
    "        averages[name] = avg_param\n",
    "        \n",
    "    # Set average in base model.\n",
    "    for name, param in base_model.named_parameters():\n",
    "        idx = indices[name].to(param.data.device)\n",
    "        avg_param = averages[name]\n",
    "        avg_param = avg_param.to(param.data.dtype)\n",
    "        avg_param = avg_param.to(param.data.device)\n",
    "        param.data.view(-1)[ idx ] = avg_param.clone()\n",
    "        if param.data.view(-1)[ idx ].norm().item() != avg_param.norm().item():\n",
    "            print(name,avg_param.norm().item(),param.data.view(-1)[ idx ].norm().item(), idx)\n",
    "            sys.exit()\n",
    "    \n",
    "    # Set average in miner models.\n",
    "    for miner_model in miner_models:\n",
    "        for name, param in miner_model.named_parameters():\n",
    "            idx = indices[name]\n",
    "            avg_param = averages[name]\n",
    "            avg_param = avg_param.to(param.data.dtype)\n",
    "            avg_param = avg_param.to(param.data.device)\n",
    "            param.data.view(-1)[ idx ] = avg_param.clone()\n",
    "\n",
    "    # Assert that all the values under the indices are the same for base_model and all models\n",
    "    for name, param in base_model.named_parameters():\n",
    "        if name not in indices: continue\n",
    "        base_values = param.view(-1)[indices[name]]\n",
    "        for miner_model in miner_models:\n",
    "            # Access miner model parameters directly\n",
    "            miner_param = dict(miner_model.named_parameters())[name]\n",
    "            miner_values = miner_param.view(-1)[indices[name]]\n",
    "            if not torch.allclose(base_values, miner_values, atol=1e-4):\n",
    "                print(f\"Difference for {name}: {base_values - miner_values}\")\n",
    "\n",
    "    # Training phase for miner models\n",
    "    for i in tqdm(range(len(miner_models)), desc=\"Training miners\"):\n",
    "        # Each miner trains on its own unique page\n",
    "        pages = await AsyncSubsetFineWebEdu2Loader.next_pages(\n",
    "            offset = offset,\n",
    "            n_pages=1,\n",
    "            seed=i\n",
    "        )\n",
    "        dataset = await AsyncSubsetFineWebEdu2Loader.create(\n",
    "            batch_size=2,\n",
    "            sequence_length=1024,\n",
    "            pages_info=pages,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # Zero gradients for the current miner model\n",
    "        miner_models[i].zero_grad()\n",
    "\n",
    "        # Train the miner model on its dataset\n",
    "        for batch in dataset:\n",
    "            input_ids = torch.tensor(batch, dtype=torch.long).to(miner_models[i].device)\n",
    "            labels = input_ids.clone()\n",
    "            labels = torch.where(labels == tokenizer.pad_token_id, -100, labels)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast(device_type=miner_models[i].device.type, dtype=torch.bfloat16):\n",
    "                outputs = miner_models[i](input_ids=input_ids, labels=labels)\n",
    "            # Backward pass\n",
    "            outputs.loss.backward()\n",
    "\n",
    "        # Update the miner model parameters\n",
    "        optimizers[i].step()\n",
    "        optimizers[i].zero_grad()\n",
    "\n",
    "    # Validation phase: compute similarities\n",
    "    for i in range(len(miner_models)):\n",
    "        # Decide whether to use a random page or the same page the miner trained on\n",
    "        is_random = True if i % 2 != 0 else False\n",
    "        seed = random.randint(0, 10000) if is_random else i\n",
    "\n",
    "        # Load validation data\n",
    "        pages = await AsyncSubsetFineWebEdu2Loader.next_pages(\n",
    "            offset=offset,\n",
    "            n_pages=1,\n",
    "            seed=seed\n",
    "        )\n",
    "        dataset = await AsyncSubsetFineWebEdu2Loader.create(\n",
    "            batch_size=2,\n",
    "            sequence_length=1024,\n",
    "            pages_info=pages,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # Zero gradients for the base model\n",
    "        base_model.zero_grad()\n",
    "\n",
    "        # Compute gradients on the base model\n",
    "        for batch in dataset:\n",
    "            input_ids = torch.tensor(batch, dtype=torch.long).to(base_model.device)\n",
    "            labels = input_ids.clone()\n",
    "            labels = torch.where(labels == tokenizer.pad_token_id, -100, labels)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast(device_type=base_model.device.type, dtype=torch.bfloat16):\n",
    "                outputs = base_model(input_ids=input_ids, labels=labels)\n",
    "            # Backward pass\n",
    "            outputs.loss.backward()\n",
    "\n",
    "        # Compute cosine similarity between the miner's update and the base model's gradient\n",
    "        total_sim = 0.0\n",
    "        total_divergence = 0.0\n",
    "        for name, param in base_model.named_parameters():\n",
    "            if param.grad is None:\n",
    "                continue  # Skip parameters without gradients\n",
    "            \n",
    "            idxs = indices[name].clone().detach().to(base_model.device)\n",
    "\n",
    "            # Get the gradient from the base model\n",
    "            g = param.grad.view(-1).clone().detach().to(base_model.device)\n",
    "\n",
    "            # Get the parameter difference (delta) between the miner model and the base model\n",
    "            p1 = miner_models[i].state_dict()[name].view(-1).clone().detach().to(base_model.device)\n",
    "            p2 = param.data.view(-1).clone().detach().to(base_model.device)\n",
    "            delta = p1 - p2\n",
    "\n",
    "            # Compute the cosine similarity between delta and gradient\n",
    "            sim = torch.nn.functional.cosine_similarity(-delta[idxs], g[idxs], dim=0).item()\n",
    "            weight = p1.data.view(-1)[idxs].norm().item() + 1e-8\n",
    "            total_sim += weight * sim\n",
    "            total_divergence += delta[idxs].norm().item()\n",
    "        \n",
    "        # Log the similarity result\n",
    "        logger.info(f'Model {i}, is_random: {is_random}, total_sim: {total_sim}, total_divergence: {total_divergence}')\n",
    "        similarity.append(total_sim)\n",
    "        divergence.append( total_divergence )\n",
    "\n",
    "    # Return the similarities\n",
    "    return similarity, divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(n_steps:int = 2 , n_models: int = 2, device: str = 'cuda:4', compression:int = 1):\n",
    "    \"\"\"\n",
    "    Main function to run the simulation over multiple steps.\n",
    "    \"\"\"\n",
    "    # Load the base model\n",
    "    logger.info(\"Loading base model...\")\n",
    "    base_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "    # Load the miner models\n",
    "    logger.info(\"Loading miner models...\")\n",
    "    miner_models = [GPT2LMHeadModel.from_pretrained('gpt2').to(device) for _ in range(n_models)]\n",
    "\n",
    "    # Create optimizers for miner models\n",
    "    optimizers = []\n",
    "    for i in range(n_models):\n",
    "        optimizers.append(optim.AdamW(\n",
    "            miner_models[i].parameters(),\n",
    "            lr=5e-5,  # Peak learning rate\n",
    "            betas=(0.9, 0.95),  # B1 and B2\n",
    "            weight_decay=0.1,  # Weight decay\n",
    "            foreach=True,  # More memory usage, but faster\n",
    "        ))\n",
    "\n",
    "    # Load the tokenizer\n",
    "    logger.info(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2', verbose=False, clean_up_tokenization_spaces=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "    # Run the simulation for multiple steps\n",
    "    results = []\n",
    "    div_results = []\n",
    "    for step in range(n_steps):\n",
    "        logger.info(f\"Starting simulation step {step+1}/{n_steps}...\")\n",
    "        sim, div = await run_sim(base_model, miner_models, optimizers, tokenizer, compression)\n",
    "        results.append(sim)\n",
    "        div_results.append(div)\n",
    "\n",
    "    # After simulation, process and plot the results\n",
    "    logger.info(\"Simulation completed. Processing results...\")\n",
    "\n",
    "    # Process similarity results into DataFrame\n",
    "    df_sim = pd.DataFrame(results, columns=[f'Model_{i}' for i in range(n_models)])\n",
    "    df_sim['Step'] = range(1, n_steps + 1)\n",
    "    df_sim = df_sim.set_index('Step')\n",
    "\n",
    "    # Process divergence results into DataFrame\n",
    "    df_div = pd.DataFrame(div_results, columns=[f'Model_{i}' for i in range(n_models)])\n",
    "    df_div['Step'] = range(1, n_steps + 1)\n",
    "    df_div = df_div.set_index('Step')\n",
    "\n",
    "    # Add is_random information\n",
    "    is_random_list = ['Random' if i % 2 != 0 else 'Same' for i in range(n_models)]\n",
    "    df_sim.columns = pd.MultiIndex.from_tuples(zip(is_random_list, df_sim.columns))\n",
    "    df_div.columns = pd.MultiIndex.from_tuples(zip(is_random_list, df_div.columns))\n",
    "\n",
    "    # Plot the similarities and divergences as subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Plot the similarities\n",
    "    for col in df_sim.columns:\n",
    "        axs[0].plot(df_sim.index, df_sim[col], marker='o', label=f'Model {col[1]} ({col[0]})')\n",
    "    axs[0].set_xlabel('Simulation Step')\n",
    "    axs[0].set_ylabel('Total Cosine Similarity')\n",
    "    axs[0].set_title('Cosine Similarity between Miner Updates and Base Model Gradients')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot the divergences\n",
    "    for col in df_div.columns:\n",
    "        axs[1].plot(df_div.index, df_div[col], marker='o', label=f'Model {col[1]} ({col[0]})')\n",
    "    axs[1].set_xlabel('Simulation Step')\n",
    "    axs[1].set_ylabel('Total Divergence')\n",
    "    axs[1].set_title('Divergence between Miner Updates and Base Model Parameters')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading base model...\n",
      "INFO:__main__:Loading miner models...\n",
      "INFO:__main__:Loading tokenizer...\n",
      "INFO:__main__:Starting simulation step 1/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 237.46708150334808, total_divergence: 0.20811511274951044\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 118.25875898876498, total_divergence: 0.20811374811455607\n",
      "INFO:__main__:Starting simulation step 2/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:12<00:00,  6.34s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 141.1485129789631, total_divergence: 0.1416793941107244\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 75.94641489913678, total_divergence: 0.14500928720349293\n",
      "INFO:__main__:Starting simulation step 3/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:12<00:00,  6.36s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 86.51070549612693, total_divergence: 0.1243679346589488\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 36.21915444517202, total_divergence: 0.1253809976510638\n",
      "INFO:__main__:Starting simulation step 4/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 84.8594133532468, total_divergence: 0.11174212662717764\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 24.808248822783195, total_divergence: 0.11254276009503883\n",
      "INFO:__main__:Starting simulation step 5/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 88.8969364477786, total_divergence: 0.09947252184542776\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 49.60713485550417, total_divergence: 0.10018855662895021\n",
      "INFO:__main__:Starting simulation step 6/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 128.94829183430872, total_divergence: 0.09179435181113149\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 83.59590756148329, total_divergence: 0.09293689687319784\n",
      "INFO:__main__:Starting simulation step 7/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 147.20770026159542, total_divergence: 0.08706968083879474\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 98.9983536436586, total_divergence: 0.08654621780306115\n",
      "INFO:__main__:Starting simulation step 8/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 142.59972641700588, total_divergence: 0.0827678750179075\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 101.25887685748538, total_divergence: 0.08241298116115559\n",
      "INFO:__main__:Starting simulation step 9/100...\n",
      "Training miners: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]\n",
      "INFO:__main__:Model 0, is_random: False, total_sim: 148.95360103434908, total_divergence: 0.07890680645709836\n",
      "INFO:__main__:Model 1, is_random: True, total_sim: 99.02260469859554, total_divergence: 0.07922407840885626\n",
      "INFO:__main__:Starting simulation step 10/100...\n",
      "Training miners:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it]"
     ]
    }
   ],
   "source": [
    "await main( n_steps = 100, n_models = 2, device = 'cuda:4', compression = 300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
